{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SLmdeyd_zlvI"
   },
   "source": [
    "# **Machine Learning for Factor Investing**\n",
    "\n",
    "This notebook implements and expands on the ideas presented in [Coqueret and Guida (2021)](http://www.mlfactor.com/), please refer to their book as it provides a really good, data-oriented introduction to quantitative equity investing. My goal in this exercise is for nothing more than my own practice and learning, and all credit goes to the original authors.\n",
    "\n",
    "I recommend running this notebook on [Google Colab](https://colab.research.google.com/), where you can up and running in only a few seconds. You might just need to download the [original data](https://github.com/shokru/mlfactor.github.io/blob/master/material/data_ml.RData) to your Google Drive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dRzqJgh50Gea"
   },
   "source": [
    "### **Chapter 0.** Importing stuff and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ozdQi6moAfN",
    "outputId": "5fcbbf56-dd51-41b0-9601-3e215ad964c5"
   },
   "outputs": [],
   "source": [
    "# Basic stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from calendar import monthrange\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\n",
    "    \"patch.facecolor\"\n",
    "] = \"white\"  # This is helpful if you're using Colab in dark mode\n",
    "plt.rcParams[\"figure.figsize\"] = 15, 7\n",
    "# import graphviz\n",
    "\n",
    "# Reading Rdata - need to install in Collab\n",
    "# !pip install pyreadr\n",
    "import pyreadr\n",
    "\n",
    "# Regression type stuff\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "from fico.portfolio import *\n",
    "from fico.evaluation import *\n",
    "\n",
    "# # Trees\n",
    "# from sklearn import tree\n",
    "# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier\n",
    "# # from xgboost import XGBClassifier\n",
    "\n",
    "# # Deep learning stuff\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TfBNEtb00R1a"
   },
   "source": [
    "### **Chapter 1.** Notations and data\n",
    "\n",
    "Mostly ingesting the data and some quick and dirty data cleaning as well as initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = pd.read_csv(\"../data/ibov_universe.csv\", index_col=None)\n",
    "\n",
    "\n",
    "universe = Portifolio(universe)\n",
    "data = universe.pre_processing()\n",
    "numeric_columns = universe.numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigest returns:\n",
    "data[[\"ticker\", \"ret12m\"]].sort_values(by=\"ret12m\", ascending=False).head(30)\n",
    "data = data.drop(index=[1151, 963, 964, 990, 579, 883, 868, 274])\n",
    "# data.loc[data['ticker'] == 'GGBR4',['date','ret12m','fator_cotacao','mkt_value','size','closed_price']]\n",
    "# if fator_cotacao changes than display index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"size\"] = (\n",
    "    data.groupby(\"date\")[\"mkt_value\"]\n",
    "    .apply(lambda x: (x > x.median()))\n",
    "    .reset_index(drop=True)\n",
    "    .replace({True: \"Large\", False: \"Small\"})\n",
    ")\n",
    "data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "return_by_size = data.groupby([\"year\", \"size\"])[\"ret12m\"].mean().reset_index()\n",
    "\n",
    "ax = sns.barplot(x=\"year\", y=\"ret12m\", hue=\"size\", data=return_by_size)\n",
    "ax.set(xlabel=\"\", ylabel=\"Average return\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))  # definindo o tamanho da figura\n",
    "df = data.groupby([\"year\"])[\"ticker\"].size().reset_index(name=\"count\")\n",
    "\n",
    "sns.barplot(data=df, x=\"year\", y=\"count\", width=0.8)\n",
    "plt.xlabel(\"ano\")\n",
    "plt.ylabel(\"Número de tickers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização da quantidade de ativos por data, podemos verificar que o ibov cresceu em número de ações ordinárias ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "qeu6Ijr3pGuZ",
    "outputId": "032a59fc-5d15-44e2-fb24-98d3074a39e1"
   },
   "outputs": [],
   "source": [
    "data.groupby(\"date\")[\"ticker\"].count().plot(\n",
    "    ylabel=\"n_assets\", title=\"Number of distinct assets through time\"\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos normalizar as colunas numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical columns:\n",
    "# import minmax scaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "transformer = scaler.fit(data[numeric_columns])\n",
    "data[numeric_columns] = transformer.transform(data[numeric_columns])\n",
    "\n",
    "# removendo elementos que estejam fora de 3 desvios padrões\n",
    "data = data[(np.abs(data[numeric_columns]) < 3).all(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verificando normalização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "ekWGnP5YrKrD",
    "outputId": "cde2c994-a706-424d-af30-de20bd3b5193"
   },
   "outputs": [],
   "source": [
    "data.loc[(data[\"date\"] == \"2022-12-29\"), \"mkt_value\"].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkoqZEVzqwQv"
   },
   "outputs": [],
   "source": [
    "features = data.columns.to_list()[3:-7]\n",
    "features_short = [\n",
    "    \"ebit12m\",\n",
    "    \"ativo_total\",\n",
    "    \"net_worth\",\n",
    "    \"liq_corr\",\n",
    "    \"vol12m\",\n",
    "    \"mkt_value\",\n",
    "    \"entreprise_value\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separação de amostra de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find separation_date:\n",
    "separation_date = data[\"date\"].unique()[int(len(data[\"date\"].unique()) * 0.8)]\n",
    "print(f\"separation_date: {separation_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx3GCgILu7jw"
   },
   "outputs": [],
   "source": [
    "separation_mask = data[\"date\"] < separation_date\n",
    "\n",
    "training_sample = data.loc[separation_mask]\n",
    "testing_sample = data.loc[~separation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfIIS69hvxNx"
   },
   "outputs": [],
   "source": [
    "stock_ids = data[\"ticker\"].unique().tolist()\n",
    "\n",
    "max_dates = data.groupby(\"ticker\")[\"date\"].count().max()\n",
    "stocks_with_max_dates = data.groupby(\"ticker\")[\"date\"].count() == max_dates\n",
    "stock_ids_short = (\n",
    "    stocks_with_max_dates.where(stocks_with_max_dates).dropna().index.tolist()\n",
    ")  # these are stocks who have data for all timestamps\n",
    "\n",
    "returns = data[data[\"ticker\"].isin(stock_ids_short)][[\"date\", \"ticker\", \"ret1m\"]]\n",
    "returns = returns.pivot(index=\"date\", columns=\"ticker\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "14J11TVD0kc-"
   },
   "source": [
    "## Factor Investing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agrupar os dados do nosso universo e realizar a segmentação das ações por tamanho, no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"mkt_value\"].describe()\n",
    "# data.groupby('date')['VM'].apply(lambda x: (x > x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "3ScBMoF11QwF",
    "outputId": "192b9c73-619b-4ab2-ab72-9ec688145b4b"
   },
   "outputs": [],
   "source": [
    "data[\"size\"] = (\n",
    "    data.groupby(\"date\")[\"mkt_value\"]\n",
    "    .apply(lambda x: (x > x.median()))\n",
    "    .reset_index(drop=True)\n",
    "    .replace({True: \"Large\", False: \"Small\"})\n",
    ")\n",
    "data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "return_by_size = data.groupby([\"year\", \"size\"])[\"ret12m\"].mean().reset_index()\n",
    "\n",
    "ax = sns.barplot(x=\"year\", y=\"ret12m\", hue=\"size\", data=return_by_size)\n",
    "ax.set(xlabel=\"\", ylabel=\"Average return\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"ticker\", \"ret12m\", \"date\"]].sort_values(by=\"ret12m\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os fatores:\n",
    "ff_factors = pd.read_csv(\"../data/risk_factors/factors.csv\", index_col=None)\n",
    "# ajustando os tipos das colunas:\n",
    "ff_factors[\"date\"] = pd.to_datetime(ff_factors[\"date\"])\n",
    "# ff_factors['date'] = ff_factors['date'].dt.strftime(\"%Y/%m/%d\")\n",
    "ff_factors.rename({\"Risk_free\": \"RF\", \"Rm_minus_Rf\": \"MKT_RF\"}, axis=1, inplace=True)\n",
    "columns_to_float = ff_factors.columns[1:]\n",
    "ff_factors[columns_to_float] = ff_factors[columns_to_float].astype(float)\n",
    "display(ff_factors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "pOlnNeZ3ueRd",
    "outputId": "36767646-07da-46fb-e6ee-ff3aaf7df913"
   },
   "outputs": [],
   "source": [
    "temp_factors = ff_factors.copy()\n",
    "\n",
    "temp_factors[\"date\"] = temp_factors[\"date\"].dt.year\n",
    "temp_factors = pd.melt(temp_factors, id_vars=\"date\")\n",
    "temp_factors = temp_factors.groupby([\"date\", \"variable\"]).mean().reset_index()\n",
    "\n",
    "plt = sns.lineplot(x=\"date\", y=\"value\", hue=\"variable\", data=temp_factors)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.7), loc=2, borderaxespad=0.0)\n",
    "plt.set_title(\"Average returns over time of common factors\");\n",
    "# replicating this from the book for completeness only, but i think it's a pretty messy chart\n",
    "# it's hard to take much insight from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "98Fhent-pb3y",
    "outputId": "05dbe957-7313-4199-9a4f-ab736885fcac"
   },
   "outputs": [],
   "source": [
    "# let's see how factors cumulative performance over time\n",
    "# but wrap that in a function that allows you to choose the start period (as that influences cumulative performance a lot)\n",
    "\n",
    "\n",
    "def plot_cumulative_performance(df, start_date=None):\n",
    "    # this function will plot cumulative performance for any wide dataframe of returns (e.g. index is date, columns are assets/factor)\n",
    "    # optional: you can pass the start date in %m/%d/%y format e.g. '1/1/1995', '12/15/2000'\n",
    "    # if you don't pass a start date, it will use the whole sample\n",
    "\n",
    "    cumul_returns = (1 + df.set_index(\"date\")).cumprod()\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = cumul_returns.index.min()\n",
    "    else:\n",
    "        start_date = datetime.strptime(start_date, \"%m/%d/%Y\")\n",
    "        cumul_returns = cumul_returns.loc[cumul_returns.index >= start_date]\n",
    "\n",
    "    first_line = pd.DataFrame(\n",
    "        [[1.0 for col in cumul_returns.columns]],\n",
    "        columns=cumul_returns.columns,\n",
    "        index=[start_date - relativedelta(months=1)],\n",
    "    )\n",
    "\n",
    "    cumul_returns = pd.concat([first_line, cumul_returns])\n",
    "\n",
    "    return cumul_returns.plot(\n",
    "        title=f'Cumulative factor performance since {start_date.strftime(\"%B %Y\")}'\n",
    "    )\n",
    "\n",
    "\n",
    "plot_cumulative_performance(ff_factors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oiNPjtKWbAxe"
   },
   "source": [
    "Below we perform **Fama-Macbeth regressions**, which is the standard way of validating a factor's risk premium in the cross-section of stock returns. As we will see, this involves a two-step process of:\n",
    "\n",
    "*   Time-series regression: regress each asset's returns on factors, i.e. one regression per asset. Store the coefficients.\n",
    "*   Cross-section regression: regress each asset's returns on coefficients obtained in previous step, i.e. one regression per time period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "skLl8esqxUeU",
    "outputId": "7bb66fe6-fef0-4a9f-d1d6-98e9c13dcb63"
   },
   "outputs": [],
   "source": [
    "# merging and cleaning up the data before we run the regressions\n",
    "data_fm = data[[\"date\", \"ticker\", \"ret1m\"]][data[\"ticker\"].isin(stock_ids_short)]\n",
    "data_fm = data_fm.merge(ff_factors, on=\"date\")\n",
    "data_fm[\"ret1m\"] = data_fm.groupby(\"ticker\")[\"ret1m\"].shift(1)\n",
    "data_fm.dropna(inplace=True)\n",
    "\n",
    "# running time series regressions\n",
    "\n",
    "reg_output = {}\n",
    "\n",
    "for k, g in data_fm.groupby(\"ticker\"):\n",
    "    model = ols(\"ret1m ~ MKT_RF + SMB + HML + WML + IML\", data=g)\n",
    "    results = model.fit()\n",
    "\n",
    "    reg_output[k] = results.params\n",
    "\n",
    "betas = pd.DataFrame.from_dict(reg_output).T\n",
    "betas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ElUn6HOg2qCi",
    "outputId": "2eccd612-1a45-4463-e6ed-20677ed06cd9"
   },
   "outputs": [],
   "source": [
    "# prepping coeficient data to run second round of regressions\n",
    "loadings = betas.drop(\"Intercept\", axis=1).reset_index(drop=True)\n",
    "ret = returns.T.reset_index(drop=True)\n",
    "fm_data = pd.concat([loadings, ret], axis=1)\n",
    "fm_data = pd.melt(fm_data, id_vars=[\"MKT_RF\", \"SMB\", \"HML\", \"WML\", \"IML\"])\n",
    "\n",
    "# running cross section regressions\n",
    "\n",
    "reg_output_2 = {}\n",
    "\n",
    "for k, g in fm_data.groupby(\"variable\"):\n",
    "    model = ols(\"value ~ MKT_RF + SMB + HML + WML + IML\", data=g)\n",
    "    results = model.fit()\n",
    "\n",
    "    reg_output_2[k] = results.params\n",
    "\n",
    "# refer to the mlfactor book or the fama-macbeth literature for more info on what the gammas stand for\n",
    "# but you can think of them as an estimate of a given factor's risk premium at a point in time\n",
    "gammas = (\n",
    "    pd.DataFrame.from_dict(reg_output_2)\n",
    "    .T.reset_index()\n",
    "    .rename({\"index\": \"date\"}, axis=1)\n",
    ")\n",
    "gammas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "qwoBZ1TX5wE0",
    "outputId": "7e5ec21f-7fa0-45f7-9079-e22d217165df"
   },
   "outputs": [],
   "source": [
    "# since we get one estimate of that risk premium for each time step, we can plot how it evolves over time\n",
    "x = pd.melt(gammas.drop(\"Intercept\", axis=1), id_vars=\"date\")\n",
    "\n",
    "g = sns.FacetGrid(x, col=\"variable\")\n",
    "g.map(sns.lineplot, \"date\", \"value\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj4S2POPfHHp"
   },
   "source": [
    "Below we deploy the factor competition strategy outlined in the book. The main idea here is to regress a factor on the remaining factors and test whether the coefficient is significant. A significant coefficient means that the factors on the right-hand side don't completely explain the factor on the left-hand side, which naturally means the latter is useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PWROYGUb9A4T",
    "outputId": "89d93729-9df0-4361-d5b0-3e6a6de47515"
   },
   "outputs": [],
   "source": [
    "factor_comp = pd.melt(ff_factors.drop(\"RF\", axis=1), id_vars=\"date\")\n",
    "\n",
    "factor_comp = factor_comp.merge(ff_factors.drop(\"RF\", axis=1), on=\"date\")\n",
    "\n",
    "factor_comp_coefs = {}\n",
    "factor_comp_tstats = {}\n",
    "\n",
    "for k, g in factor_comp.groupby(\"variable\"):\n",
    "    reg_data = g.drop([k, \"date\", \"variable\"], axis=1)\n",
    "    formula = \"value ~ \" + \" + \".join(reg_data.columns.values[1:].tolist())\n",
    "\n",
    "    model = ols(formula, data=reg_data)\n",
    "    results = model.fit()\n",
    "\n",
    "    factor_comp_coefs[k] = results.params\n",
    "    factor_comp_tstats[k] = results.tvalues\n",
    "\n",
    "alphas = pd.DataFrame.from_dict(factor_comp_coefs).T\n",
    "alphas_tstats = pd.DataFrame.from_dict(factor_comp_tstats).T\n",
    "\n",
    "alphas_tstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6gGxAQt0EYMO",
    "outputId": "c108556d-accd-4637-ee7a-51db6bc9bcf8"
   },
   "outputs": [],
   "source": [
    "alphas_table = alphas.round(3).applymap(str)\n",
    "\n",
    "prob99 = 2.58\n",
    "prob95 = 1.96\n",
    "\n",
    "alphas_table[alphas_tstats >= prob99] = alphas_table[alphas_tstats >= prob99] + \" (**)\"\n",
    "alphas_table[alphas_tstats.apply(lambda x: x.between(prob95, prob99))] = (\n",
    "    alphas_table[alphas_tstats.apply(lambda x: x.between(prob95, prob99))] + \" (*)\"\n",
    ")\n",
    "\n",
    "factors = factor_comp.columns[3:].tolist()\n",
    "\n",
    "alphas_table = alphas_table[[\"Intercept\"] + factors].reindex(factors)\n",
    "\n",
    "alphas_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3vyiXTi0gOf3"
   },
   "source": [
    "Below we explore factor time series momentum by looking at the partial auto-correlation functions. The shaded lines are confidence intervals, which the *statsmodels* library handily calculates for us by default (at 5% confidence levels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "wk5ClzvnKrV2",
    "outputId": "394dd83c-0826-4aff-990e-f6c07e6f3895"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "nfactors = len(factors)\n",
    "\n",
    "# Plot y from -0.1 to 0.1:\n",
    "fig, axs = plt.subplots(ncols=nfactors, figsize=(20, 2))\n",
    "\n",
    "for i, factor in enumerate(factors):\n",
    "    plot_pacf(ff_factors[factor], ax=axs[i], lags=20, zero=False, title=factor)\n",
    "    axs[i].set_ylim(-0.1, 0.1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6V07Btxwjm/oW8AQiQ+f5",
   "collapsed_sections": [],
   "mount_file_id": "1Eire9PNPHV84V51fkKAoITcXKpv582qu",
   "name": "py-mlfactor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
